{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from model import YourRegressionModel  # Import your regression model\n",
    "# from packnet.src.dataset import get_data_loaders  # Function to get train/test loaders\n",
    "\n",
    "from packnet.src.prune import SparsePruner  # Import the pruner\n",
    "# from packnet import Manager  # Import the modified Manager c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packnet.src import dataset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "class Manager(object):\n",
    "    \"\"\"Handles training and pruning.\"\"\"\n",
    "\n",
    "    def __init__(self, args, model, previous_masks, dataset2idx, dataset2biases):\n",
    "        self.args = args\n",
    "        self.cuda = args.cuda\n",
    "        self.model = model\n",
    "        self.dataset2idx = dataset2idx\n",
    "        self.dataset2biases = dataset2biases\n",
    "\n",
    "        if args.mode != 'check':\n",
    "            # Set up data loader, criterion, and pruner.\n",
    "            if 'cropped' in args.train_path:\n",
    "                train_loader = dataset.train_loader_cropped\n",
    "                test_loader = dataset.test_loader_cropped\n",
    "            else:\n",
    "                train_loader = dataset.train_loader\n",
    "                test_loader = dataset.test_loader\n",
    "            self.train_data_loader = train_loader(\n",
    "                args.train_path, args.batch_size, pin_memory=args.cuda)\n",
    "            self.test_data_loader = test_loader(\n",
    "                args.test_path, args.batch_size, pin_memory=args.cuda)\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            self.pruner = SparsePruner(\n",
    "                self.model, self.args.prune_perc_per_layer, previous_masks,\n",
    "                self.args.train_biases, self.args.train_bn)\n",
    "\n",
    "    def eval(self, dataset_idx, biases=None):\n",
    "        \"\"\"Performs evaluation.\"\"\"\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.apply_mask(dataset_idx)\n",
    "        if biases is not None:\n",
    "            self.pruner.restore_biases(biases)\n",
    "\n",
    "        self.model.eval()\n",
    "        error_meter = None\n",
    "\n",
    "        print('Performing eval...')\n",
    "        for batch, label in tqdm(self.test_data_loader, desc='Eval'):\n",
    "            if self.cuda:\n",
    "                batch = batch.cuda()\n",
    "            batch = Variable(batch, volatile=True)\n",
    "\n",
    "            output = self.model(batch)\n",
    "\n",
    "            # Init error meter.\n",
    "            if error_meter is None:\n",
    "                topk = [1]\n",
    "                if output.size(1) > 5:\n",
    "                    topk.append(5)\n",
    "                error_meter = tnt.meter.ClassErrorMeter(topk=topk)\n",
    "            error_meter.add(output.data, label)\n",
    "\n",
    "        errors = error_meter.value()\n",
    "        print('Error: ' + ', '.join('@%s=%.2f' %\n",
    "                                    t for t in zip(topk, errors)))\n",
    "        if self.args.train_bn:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.train_nobn()\n",
    "        return errors\n",
    "\n",
    "    def do_batch(self, optimizer, batch, label):\n",
    "        \"\"\"Runs model for one batch.\"\"\"\n",
    "        if self.cuda:\n",
    "            batch = batch.cuda()\n",
    "            label = label.cuda()\n",
    "        batch = Variable(batch)\n",
    "        label = Variable(label)\n",
    "\n",
    "        # Set grads to 0.\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Do forward-backward.\n",
    "        output = self.model(batch)\n",
    "        self.criterion(output, label).backward()\n",
    "\n",
    "        # Set fixed param grads to 0.\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.make_grads_zero()\n",
    "\n",
    "        # Update params.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Set pruned weights to 0.\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.make_pruned_zero()\n",
    "\n",
    "    def do_epoch(self, epoch_idx, optimizer):\n",
    "        \"\"\"Trains model for one epoch.\"\"\"\n",
    "        for batch, label in tqdm(self.train_data_loader, desc='Epoch: %d ' % (epoch_idx)):\n",
    "            self.do_batch(optimizer, batch, label)\n",
    "\n",
    "    def save_model(self, epoch, best_accuracy, errors, savename):\n",
    "        \"\"\"Saves model to file.\"\"\"\n",
    "        base_model = self.model\n",
    "\n",
    "        # Prepare the ckpt.\n",
    "        self.dataset2idx[self.args.dataset] = self.pruner.current_dataset_idx\n",
    "        self.dataset2biases[self.args.dataset] = self.pruner.get_biases()\n",
    "        ckpt = {\n",
    "            'args': self.args,\n",
    "            'epoch': epoch,\n",
    "            'accuracy': best_accuracy,\n",
    "            'errors': errors,\n",
    "            'dataset2idx': self.dataset2idx,\n",
    "            'previous_masks': self.pruner.current_masks,\n",
    "            'model': base_model,\n",
    "        }\n",
    "        if self.args.train_biases:\n",
    "            ckpt['dataset2biases'] = self.dataset2biases\n",
    "\n",
    "        # Save to file.\n",
    "        torch.save(ckpt, savename + '.pt')\n",
    "\n",
    "    def train(self, epochs, optimizer, save=True, savename='', best_accuracy=0):\n",
    "        \"\"\"Performs training.\"\"\"\n",
    "        best_accuracy = best_accuracy\n",
    "        error_history = []\n",
    "\n",
    "        if self.args.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        for idx in range(epochs):\n",
    "            epoch_idx = idx + 1\n",
    "            print('Epoch: %d' % (epoch_idx))\n",
    "\n",
    "            optimizer = utils.step_lr(epoch_idx, self.args.lr, self.args.lr_decay_every,\n",
    "                                      self.args.lr_decay_factor, optimizer)\n",
    "            if self.args.train_bn:\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.train_nobn()\n",
    "            self.do_epoch(epoch_idx, optimizer)\n",
    "            errors = self.eval(self.pruner.current_dataset_idx)\n",
    "            error_history.append(errors)\n",
    "            accuracy = 100 - errors[0]  # Top-1 accuracy.\n",
    "\n",
    "            # Save performance history and stats.\n",
    "            with open(savename + '.json', 'w') as fout:\n",
    "                json.dump({\n",
    "                    'error_history': error_history,\n",
    "                    'args': vars(self.args),\n",
    "                }, fout)\n",
    "\n",
    "            # Save best model, if required.\n",
    "            if save and accuracy > best_accuracy:\n",
    "                print('Best model so far, Accuracy: %0.2f%% -> %0.2f%%' %\n",
    "                      (best_accuracy, accuracy))\n",
    "                best_accuracy = accuracy\n",
    "                self.save_model(epoch_idx, best_accuracy, errors, savename)\n",
    "\n",
    "        print('Finished finetuning...')\n",
    "        print('Best error/accuracy: %0.2f%%, %0.2f%%' %\n",
    "              (100 - best_accuracy, best_accuracy))\n",
    "        print('-' * 16)\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"Perform pruning.\"\"\"\n",
    "        print('Pre-prune eval:')\n",
    "        self.eval(self.pruner.current_dataset_idx)\n",
    "\n",
    "        self.pruner.prune()\n",
    "        self.check(True)\n",
    "\n",
    "        print('\\nPost-prune eval:')\n",
    "        errors = self.eval(self.pruner.current_dataset_idx)\n",
    "        accuracy = 100 - errors[0]  # Top-1 accuracy.\n",
    "        self.save_model(-1, accuracy, errors,\n",
    "                        self.args.save_prefix + '_postprune')\n",
    "\n",
    "        # Do final finetuning to improve results on pruned network.\n",
    "        if self.args.post_prune_epochs:\n",
    "            print('Doing some extra finetuning...')\n",
    "            optimizer = optim.SGD(self.model.parameters(),\n",
    "                                  lr=self.args.lr, momentum=0.9,\n",
    "                                  weight_decay=self.args.weight_decay)\n",
    "            self.train(self.args.post_prune_epochs, optimizer, save=True,\n",
    "                       savename=self.args.save_prefix + '_final', best_accuracy=accuracy)\n",
    "\n",
    "        print('-' * 16)\n",
    "        print('Pruning summary:')\n",
    "        self.check(True)\n",
    "        print('-' * 16)\n",
    "\n",
    "    def check(self, verbose=False):\n",
    "        \"\"\"Makes sure that the layers are pruned.\"\"\"\n",
    "        print('Checking...')\n",
    "        for layer_idx, module in enumerate(self.model.shared.modules()):\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.data\n",
    "                num_params = weight.numel()\n",
    "                num_zero = weight.view(-1).eq(0).sum()\n",
    "                if verbose:\n",
    "                    print('Layer #%d: Pruned %d/%d (%.2f%%)' %\n",
    "                          (layer_idx, num_zero, num_params, 100 * num_zero / num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.hidden = nn.Linear(1, 64)  # One hidden layer with 10 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(64, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\n",
    "    \"task1\": {\"train_path\": \"data/task1_train\", \"test_path\": \"data/task1_test\"},\n",
    "    \"task2\": {\"train_path\": \"data/task2_train\", \"test_path\": \"data/task2_test\"},\n",
    "    \"task3\": {\"train_path\": \"data/task3_train\", \"test_path\": \"data/task3_test\"}\n",
    "}\n",
    "\n",
    "# Initialize the model (shared across tasks)\n",
    "model = RegressionModel()\n",
    "\n",
    "# Initialize masks and bookkeeping\n",
    "previous_masks = None\n",
    "dataset2idx = {}  # Persistent index mapping\n",
    "dataset2biases = {}  # Persistent biases mapping\n",
    "\n",
    "# Iterate over tasks\n",
    "for task_idx, (task_name, paths) in enumerate(tasks.items()):\n",
    "    print(f\"\\n=== Training on {task_name} ===\")\n",
    "\n",
    "    # Reinitialize optimizer per task to avoid state leakage\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Define arguments for this task\n",
    "    args = {\n",
    "        \"train_path\": paths[\"train_path\"],\n",
    "        \"test_path\": paths[\"test_path\"],\n",
    "        \"batch_size\": 32,\n",
    "        \"cuda\": torch.cuda.is_available(),\n",
    "        \"mode\": \"train\",\n",
    "        \"disable_pruning_mask\": False,\n",
    "        \"train_bn\": False,\n",
    "        \"prune_perc_per_layer\": 0.2,  # Fraction of neurons to prune\n",
    "        \"train_biases\": True,\n",
    "        \"lr\": 0.001,\n",
    "        \"lr_decay_every\": 10,\n",
    "        \"lr_decay_factor\": 0.1,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"post_prune_epochs\": 5,  # Retraining epochs after pruning\n",
    "        \"save_prefix\": f\"checkpoint_{task_name}\"\n",
    "    }\n",
    "\n",
    "    # Convert dictionary to an object mimicking argparse.Namespace\n",
    "    class ArgsObj:\n",
    "        def __init__(self, **kwargs):\n",
    "            self.__dict__.update(kwargs)\n",
    "\n",
    "    args = ArgsObj(**args)\n",
    "\n",
    "    # Initialize the Manager for this task\n",
    "    manager = Manager(args, model, previous_masks, dataset2idx, dataset2biases)\n",
    "\n",
    "    # Ensure that dataset2idx and dataset2biases persist\n",
    "    assert manager.dataset2idx is dataset2idx, \"dataset2idx not persisting correctly!\"\n",
    "    assert manager.dataset2biases is dataset2biases, \"dataset2biases not persisting correctly!\"\n",
    "\n",
    "    # Step 1: Train with all available parameters\n",
    "    print(f\"Training {task_name} with all available parameters...\")\n",
    "    manager.train(epochs=50, optimizer=optimizer, save=True, savename=f\"model_{task_name}\")\n",
    "\n",
    "    # Step 2: Prune model\n",
    "    print(f\"Pruning {task_name} to remove redundant neurons...\")\n",
    "    manager.prune()\n",
    "\n",
    "    # Step 3: Retrain (Finetune) using only assigned neurons\n",
    "    print(f\"Retraining {task_name} using only the pruned neurons...\")\n",
    "    manager.train(epochs=args.post_prune_epochs, optimizer=optimizer, save=True, savename=f\"model_{task_name}_finetuned\")\n",
    "\n",
    "    # Step 4: Store the pruning masks to ensure future tasks don't use pruned neurons\n",
    "    previous_masks = manager.pruner.current_masks\n",
    "\n",
    "print(\"Training, pruning, and retraining completed for all tasks!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
