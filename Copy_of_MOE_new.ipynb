{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruC5_Zno7BIl",
        "outputId": "cfc16351-bf05-4a63-8f8e-0c3654a98169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KQ4l9Vwdqa8M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Feature Extractor (ResNet-18)\n",
        "# ----------------------------\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Gating Network\n",
        "# ----------------------------\n",
        "# class GatingNetwork(nn.Module):\n",
        "#     def __init__(self, input_dim, num_experts):\n",
        "#         super(GatingNetwork, self).__init__()\n",
        "#         self.fc = nn.Linear(input_dim, num_experts)\n",
        "\n",
        "#     # def forward(self, x):\n",
        "#     #     logits = self.fc(x)\n",
        "#     #     # gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n",
        "#     #     expert_scores = F.softmax((logits), dim=1)\n",
        "#     #     expert_id = torch.argmax(expert_scores, dim=1)\n",
        "#     #     return expert_id, expert_scores\n",
        "\n",
        "#     def forward(self, x):\n",
        "#       logits = self.fc(x)\n",
        "#       gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n",
        "#       expert_scores = F.softmax((logits + gumbel_noise) / 0.5, dim=-1)\n",
        "#       # print(expert_scores)\n",
        "#       # print(\"expert_scores: \",expert_scores)\n",
        "#       return expert_scores  # shape [B, num_experts]\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Masked Feature Extractor\n",
        "# ----------------------------\n",
        "# class MaskedLUFeatureExtractor(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_experts, threshold=0.5, num_layers=4):\n",
        "#         super(MaskedLUFeatureExtractor, self).__init__()\n",
        "#         self.num_experts = num_experts\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.threshold = threshold\n",
        "\n",
        "#         # Create real linear layers (separate per expert and layer)\n",
        "#         self.linear_layers = nn.ModuleDict()\n",
        "\n",
        "#         # Create L, U, and b for mask generation\n",
        "#         self.L_matrices = nn.ParameterDict()\n",
        "#         self.U_matrices = nn.ParameterDict()\n",
        "#         self.bias_masks = nn.ParameterDict()\n",
        "\n",
        "#         for expert_id in range(num_experts):\n",
        "#             for layer_idx in range(num_layers):\n",
        "#                 d_in = input_dim if layer_idx == 0 else hidden_dim\n",
        "#                 d_out = hidden_dim\n",
        "#                 k = max(d_in, d_out)\n",
        "\n",
        "#                 # Initialize linear layer for this expert and layer\n",
        "#                 lin_key = f\"expert{expert_id}_layer{layer_idx}_linear\"\n",
        "#                 self.linear_layers[lin_key] = nn.Linear(d_in, d_out)\n",
        "\n",
        "#                 # L, U, and bias mask\n",
        "#                 l_key = f\"expert{expert_id}_layer{layer_idx}_L\"\n",
        "#                 u_key = f\"expert{expert_id}_layer{layer_idx}_U\"\n",
        "#                 b_key = f\"expert{expert_id}_layer{layer_idx}_b\"\n",
        "\n",
        "#                 self.L_matrices[l_key] = nn.Parameter(torch.tril(torch.randn(d_out, k)))\n",
        "#                 self.U_matrices[u_key] = nn.Parameter(torch.randn(k, d_in))\n",
        "#                 self.bias_masks[b_key] = nn.Parameter(torch.zeros(d_out))\n",
        "\n",
        "#     def forward(self, x, expert_id):\n",
        "#         for i in range(self.num_layers):\n",
        "#             lin_key = f\"expert{expert_id}_layer{i}_linear\"\n",
        "#             l_key = f\"expert{expert_id}_layer{i}_L\"\n",
        "#             u_key = f\"expert{expert_id}_layer{i}_U\"\n",
        "#             b_key = f\"expert{expert_id}_layer{i}_b\"\n",
        "\n",
        "#             linear = self.linear_layers[lin_key]\n",
        "#             weight = linear.weight         # [d_out, d_in]\n",
        "#             bias = linear.bias             # [d_out]\n",
        "\n",
        "#             # Generate mask using sigmoid(50 * L @ U)\n",
        "#             L = self.L_matrices[l_key]     # [d_out, k]\n",
        "#             U = self.U_matrices[u_key]     # [k, d_in]\n",
        "#             b_mask = self.bias_masks[b_key]  # [d_out]\n",
        "\n",
        "#             W_mask = torch.sigmoid(10 * (L @ U))\n",
        "#             # print(W_mask)          # [d_out, d_in]\n",
        "#             b_mask = torch.sigmoid(b_mask)                  # [d_out]\n",
        "\n",
        "#             # Apply mask\n",
        "#             masked_weight = weight * W_mask                 # [d_out, d_in]\n",
        "#             masked_bias = bias * b_mask                     # [d_out]\n",
        "\n",
        "#             # Linear transformation with masked weights and biases\n",
        "#             x = F.linear(x, masked_weight, masked_bias)\n",
        "#             x = F.relu(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# class MaskedLUFeatureExtractor(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, num_experts, num_layers=4):\n",
        "#         super(MaskedLUFeatureExtractor, self).__init__()\n",
        "#         self.num_experts = num_experts\n",
        "#         self.num_layers = num_layers\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.input_dim = input_dim\n",
        "\n",
        "#         self.linear_layers = nn.ModuleDict()\n",
        "\n",
        "#         for expert_id in range(num_experts):\n",
        "#             for layer_idx in range(num_layers):\n",
        "#                 d_in = input_dim if layer_idx == 0 else hidden_dim\n",
        "#                 d_out = hidden_dim\n",
        "\n",
        "#                 lin_key = f\"expert{expert_id}_layer{layer_idx}_linear\"\n",
        "#                 self.linear_layers[lin_key] = nn.Linear(d_in, d_out)\n",
        "\n",
        "#                 # Create and register weight and bias masks\n",
        "#                 torch.manual_seed(expert_id * 10 + layer_idx)  # for reproducibility\n",
        "\n",
        "#                 weight_mask = torch.ones(d_out, d_in)\n",
        "#                 bias_mask = torch.ones(d_out)\n",
        "\n",
        "#                 self.register_buffer(f\"weight_mask_{expert_id}_{layer_idx}\", weight_mask)\n",
        "#                 self.register_buffer(f\"bias_mask_{expert_id}_{layer_idx}\", bias_mask)\n",
        "\n",
        "#     def forward(self, x, expert_id):\n",
        "#         for i in range(self.num_layers):\n",
        "#             lin_key = f\"expert{expert_id}_layer{i}_linear\"\n",
        "#             linear = self.linear_layers[lin_key]\n",
        "\n",
        "#             weight = linear.weight\n",
        "#             bias = linear.bias\n",
        "\n",
        "#             W_mask = getattr(self, f\"weight_mask_{expert_id}_{i}\")\n",
        "#             b_mask = getattr(self, f\"bias_mask_{expert_id}_{i}\")\n",
        "\n",
        "#             masked_weight = weight * W_mask.detach()\n",
        "#             masked_bias = bias * b_mask.detach()\n",
        "\n",
        "#             x = F.linear(x, masked_weight, masked_bias)\n",
        "#             x = F.relu(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "class MaskedLUFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_experts):\n",
        "        super(MaskedLUFeatureExtractor, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "        self.linear_layers = nn.ModuleDict()\n",
        "\n",
        "        for expert_id in range(num_experts):\n",
        "            self.linear_layers[f\"expert{expert_id}_layer0\"] = nn.Linear(input_dim, hidden_dim)\n",
        "            self.linear_layers[f\"expert{expert_id}_layer1\"] = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.linear_layers[f\"expert{expert_id}_layer2\"] = nn.Linear(hidden_dim, hidden_dim)\n",
        "            self.linear_layers[f\"expert{expert_id}_layer3\"] = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, expert_id):\n",
        "        x = F.relu(self.linear_layers[f\"expert{expert_id}_layer0\"](x))\n",
        "        x = F.relu(self.linear_layers[f\"expert{expert_id}_layer1\"](x))\n",
        "        x = F.relu(self.linear_layers[f\"expert{expert_id}_layer2\"](x))\n",
        "        x = F.relu(self.linear_layers[f\"expert{expert_id}_layer3\"](x))\n",
        "        return x\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Shared Classification Layer\n",
        "# ----------------------------\n",
        "class SharedClassifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super(SharedClassifier, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "        # return F.softmax(self.fc(x), dim=1)\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Mixture of Experts\n",
        "# ----------------------------\n",
        "class MixtureOfExperts(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim, num_experts=1, output_dim=200):\n",
        "        super(MixtureOfExperts, self).__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.feature_extractor = FeatureExtractor()\n",
        "        self.masked_feature_extractor = MaskedLUFeatureExtractor(feature_dim, hidden_dim, num_experts)\n",
        "        self.classifier = SharedClassifier(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "      features = self.feature_extractor(x)             # [B, feature_dim]\n",
        "      features = self.masked_feature_extractor(features, expert_id=0)  # [B, hidden_dim\n",
        "      logits = self.classifier(features)                      # [B, output_dim]\n",
        "      return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_FHvBc5kFkre"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self,data,labels):\n",
        "    self.data = data\n",
        "    self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.data[idx],self.labels[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02fIYBr5u1Ei"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sM6jvWK7qa8O"
      },
      "outputs": [],
      "source": [
        "def full_training(model, dataloader, criterion, device,optmizer, num_epochs=50, log_interval=100):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            # print(\"Outputs: \",outputs)\n",
        "            # print(\"Labels: \",labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            # print(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            # print(\"Predicted \",predicted)\n",
        "            # print(\"Labels: \",labels)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(f\"Batch [{batch_idx}/{len(dataloader)}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        # for name, param in model.named_parameters():\n",
        "        #   print(f\"{name}: requires : {param.requires_grad} | grad not None? {param.grad is not None} | grad norm: {param.grad.norm() if param.grad is not None else 'NA'}\")\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        epoch_accuracy = 100. * correct / total\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sAPPxKiaqa8P",
        "outputId": "c23a036b-f566-4517-c840-3fb4eb7fe4fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 157MB/s]\n"
          ]
        }
      ],
      "source": [
        "# --- Set device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# --- Transforms ---\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# --- Load individual datasets ---\n",
        "\n",
        "cub_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/cubs_cropped/train', transform=transform)\n",
        "# print(cub_dataset)\n",
        "\n",
        "# image_paths = [sample[0] for sample in cub_dataset.samples]\n",
        "# class_names = [cub_dataset.classes[label] for _, label in cub_dataset.samples]\n",
        "# print(cub_dataset.class_to_idx)\n",
        "# print(image_paths)\n",
        "# print(class_names)\n",
        "\n",
        "# train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# combined_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# cub_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/data', transform=transform)\n",
        "# flowers_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/flowers/train', transform=transform)\n",
        "\n",
        "\n",
        "# --- Merge datasets WITHOUT offsetting class labels ---\n",
        "# combined_dataset = ConcatDataset([cub_dataset, flowers_dataset])\n",
        "# combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "combined_loader = DataLoader(cub_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "# --- Initialize the Mixture of Experts model with the combined output dim ---\n",
        "model = MixtureOfExperts(feature_dim=512, hidden_dim=256, num_experts=1, output_dim=200).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.named_parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wy4Koo_M8bsX"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# images, labels = next(iter(combined_loader))\n",
        "\n",
        "# # Define class names if you want to display them\n",
        "# class_names = cub_dataset.classes\n",
        "\n",
        "# plt.figure(figsize=(10, 4))\n",
        "# for i in range(4):\n",
        "#     img = images[i].permute(1, 2, 0).numpy()  # convert to HWC format\n",
        "#     plt.subplot(1, 4, i + 1)\n",
        "#     plt.imshow(img)\n",
        "#     plt.title(class_names[labels[i]])\n",
        "#     plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1IOkFGKiaq6"
      },
      "outputs": [],
      "source": [
        "# --- Full Training on Merged Dataset ---\n",
        "# print(\"\\n==== Training on Combined CUB + Flowers Dataset ====\")\n",
        "full_training(model, combined_loader, criterion, device, optimizer,num_epochs=50, log_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmXT9_x-qa8P"
      },
      "outputs": [],
      "source": [
        "# save the model that has been trained in drive\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/model_4_layer_nomask.pth\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}