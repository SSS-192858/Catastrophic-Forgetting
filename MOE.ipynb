{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchnet as tnt\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "from PIL import Image\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, model_type=\"resnet18\", num_classes=10):\n",
    "        super(Expert, self).__init__()\n",
    "        if model_type == \"resnet18\":\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        elif model_type == \"vgg16\":\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            self.model.classifier[-1] = nn.Linear(self.model.classifier[-1].in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Choose 'resnet18' or 'vgg16'\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=-1)\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, n_experts, input_dim=512):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, n_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "class MOEModel(nn.Module):\n",
    "    def __init__(self, n_experts=3, model_type=\"resnet18\", num_classes=10, lr=0.001):\n",
    "        super(MOEModel, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(model_type, num_classes) for _ in range(n_experts)])\n",
    "        self.gate = Gate(n_experts, 512 if model_type == \"resnet18\" else 4096)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_outputs = self.gate(x).unsqueeze(-1)  # (batch_size, n_experts, 1)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)  # (batch_size, num_classes, n_experts)\n",
    "        return torch.matmul(expert_outputs, gate_outputs).squeeze(-1)  # (batch_size, num_classes)\n",
    "    \n",
    "    def probabilities(self, x, y):\n",
    "        expert_outputs = self.forward(x)\n",
    "        return torch.sum(y * expert_outputs, dim=1)\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        probs = self.probabilities(x, y)\n",
    "        return -torch.log(probs + 1e-9).mean()\n",
    "    \n",
    "    def grad(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.calculate_loss(x, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        loss = self.grad(x, y)\n",
    "        self.optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SparsePruner:\n",
    "    \"\"\"Performs pruning on the experts in the MOE model.\"\"\"\n",
    "\n",
    "    def __init__(self, model, prune_perc, previous_masks, train_bias, train_bn):\n",
    "        self.model = model\n",
    "        self.prune_perc = prune_perc\n",
    "        self.train_bias = train_bias\n",
    "        self.train_bn = train_bn\n",
    "        self.current_masks = None\n",
    "        self.previous_masks = previous_masks\n",
    "        \n",
    "        # Identify dataset index from the previous masks\n",
    "        valid_key = list(previous_masks.keys())[0]\n",
    "        self.current_dataset_idx = previous_masks[valid_key].max()\n",
    "    \n",
    "    def pruning_mask(self, weights, previous_mask, layer_idx):\n",
    "        \"\"\"Computes a pruning mask based on weight magnitudes.\"\"\"\n",
    "        previous_mask = previous_mask.cuda()\n",
    "        tensor = weights[previous_mask.eq(self.current_dataset_idx)]\n",
    "        abs_tensor = tensor.abs()\n",
    "        cutoff_rank = round(self.prune_perc * tensor.numel())\n",
    "        cutoff_value = abs_tensor.view(-1).cpu().kthvalue(cutoff_rank)[0].item()\n",
    "\n",
    "        remove_mask = weights.abs().le(cutoff_value) * previous_mask.eq(self.current_dataset_idx)\n",
    "        previous_mask[remove_mask.eq(1)] = 0\n",
    "        mask = previous_mask\n",
    "        \n",
    "        print(f'Layer #{layer_idx}, pruned {mask.eq(0).sum()}/{tensor.numel()} ({100 * mask.eq(0).sum() / tensor.numel():.2f}%)')\n",
    "        return mask\n",
    "    \n",
    "    def prune(self):\n",
    "        \"\"\"Prunes only the expert networks while keeping the gating network untouched.\"\"\"\n",
    "        print(f'Pruning experts for dataset idx: {self.current_dataset_idx}')\n",
    "        assert self.current_masks is None, 'Pruning twice?'\n",
    "        self.current_masks = {}\n",
    "        \n",
    "        print(f'Pruning each expert layer by removing {100 * self.prune_perc:.2f}% of values')\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    mask = self.pruning_mask(module.weight.data, self.previous_masks[(expert_idx, layer_idx)], layer_idx)\n",
    "                    self.current_masks[(expert_idx, layer_idx)] = mask.cuda()\n",
    "                    module.weight.data[self.current_masks[(expert_idx, layer_idx)].eq(0)] = 0.0\n",
    "    \n",
    "    def make_grads_zero(self):\n",
    "        \"\"\"Sets gradients of pruned weights to zero for the expert networks.\"\"\"\n",
    "        assert self.current_masks is not None\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    layer_mask = self.current_masks[(expert_idx, layer_idx)]\n",
    "                    \n",
    "                    if module.weight.grad is not None:\n",
    "                        module.weight.grad.data[layer_mask.ne(self.current_dataset_idx)] = 0\n",
    "                        if not self.train_bias and module.bias is not None:\n",
    "                            module.bias.grad.data.fill_(0)\n",
    "\n",
    "    def make_pruned_zero(self):\n",
    "        \"\"\"Forces pruned weights to remain zero in the expert networks.\"\"\"\n",
    "        assert self.current_masks is not None\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    module.weight.data[self.current_masks[(expert_idx, layer_idx)].eq(0)] = 0.0\n",
    "\n",
    "    def apply_mask(self, dataset_idx):\n",
    "        \"\"\"Applies the pruning mask for a specific dataset.\"\"\"\n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    weight = module.weight.data\n",
    "                    mask = self.previous_masks[(expert_idx, layer_idx)].cuda()\n",
    "                    weight[mask.eq(0)] = 0.0\n",
    "                    weight[mask.gt(dataset_idx)] = 0.0\n",
    "    \n",
    "    def restore_biases(self, biases):\n",
    "        \"\"\"Use the given biases to replace existing biases.\"\"\"\n",
    "        for module_idx, module in enumerate(self.model.shared.modules()):\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.copy_(biases[module_idx])\n",
    "\n",
    "    def get_biases(self):\n",
    "        \"\"\"Gets a copy of the current biases.\"\"\"\n",
    "        biases = {}\n",
    "        for module_idx, module in enumerate(self.model.shared.modules()):\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                if module.bias is not None:\n",
    "                    biases[module_idx] = module.bias.data.clone()\n",
    "        return biases\n",
    "\n",
    "\n",
    "    def make_finetuning_mask(self):\n",
    "        \"\"\"Allows previously pruned weights to be trainable for the new dataset.\"\"\"\n",
    "        assert self.previous_masks is not None\n",
    "        self.current_dataset_idx += 1\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    mask = self.previous_masks[(expert_idx, layer_idx)]\n",
    "                    mask[mask.eq(0)] = self.current_dataset_idx\n",
    "        \n",
    "        self.current_masks = self.previous_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_lr(epoch, base_lr, lr_decay_every, lr_decay_factor, optimizer):\n",
    "    \"\"\"Handles step decay of learning rate.\"\"\"\n",
    "    factor = np.power(lr_decay_factor, np.floor((epoch - 1) / lr_decay_every))\n",
    "    new_lr = base_lr * factor\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    print('Set lr to ', new_lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = argparse.ArgumentParser()\n",
    "FLAGS.add_argument('--arch',\n",
    "                   choices=['vgg16', 'vgg16bn', 'resnet50', 'densenet121'],\n",
    "                   help='Architectures')\n",
    "FLAGS.add_argument('--mode',\n",
    "                   choices=['finetune', 'prune', 'check', 'eval'],\n",
    "                   help='Run mode')\n",
    "FLAGS.add_argument('--finetune_layers',\n",
    "                   choices=['all', 'fc', 'classifier'], default='all',\n",
    "                   help='Which layers to finetune, fc only works with vgg')\n",
    "FLAGS.add_argument('--num_outputs', type=int, default=-1,\n",
    "                   help='Num outputs for dataset')\n",
    "# Optimization options.\n",
    "FLAGS.add_argument('--lr', type=float,\n",
    "                   help='Learning rate')\n",
    "FLAGS.add_argument('--lr_decay_every', type=int,\n",
    "                   help='Step decay every this many epochs')\n",
    "FLAGS.add_argument('--lr_decay_factor', type=float,\n",
    "                   help='Multiply lr by this much every step of decay')\n",
    "FLAGS.add_argument('--finetune_epochs', type=int,\n",
    "                   help='Number of initial finetuning epochs')\n",
    "FLAGS.add_argument('--batch_size', type=int, default=32,\n",
    "                   help='Batch size')\n",
    "FLAGS.add_argument('--weight_decay', type=float, default=0.0,\n",
    "                   help='Weight decay')\n",
    "# Paths.\n",
    "FLAGS.add_argument('--dataset', type=str, default='',\n",
    "                   help='Name of dataset')\n",
    "FLAGS.add_argument('--train_path', type=str, default='',\n",
    "                   help='Location of train data')\n",
    "FLAGS.add_argument('--test_path', type=str, default='',\n",
    "                   help='Location of test data')\n",
    "FLAGS.add_argument('--save_prefix', type=str, default='../checkpoints/',\n",
    "                   help='Location to save model')\n",
    "FLAGS.add_argument('--loadname', type=str, default='',\n",
    "                   help='Location to save model')\n",
    "# Pruning options.\n",
    "FLAGS.add_argument('--prune_method', type=str, default='sparse',\n",
    "                   choices=['sparse'],\n",
    "                   help='Pruning method to use')\n",
    "FLAGS.add_argument('--prune_perc_per_layer', type=float, default=0.5,\n",
    "                   help='% of neurons to prune per layer')\n",
    "FLAGS.add_argument('--post_prune_epochs', type=int, default=0,\n",
    "                   help='Number of epochs to finetune for after pruning')\n",
    "FLAGS.add_argument('--disable_pruning_mask', action='store_true', default=False,\n",
    "                   help='use masking or not')\n",
    "FLAGS.add_argument('--train_biases', action='store_true', default=False,\n",
    "                   help='use separate biases or not')\n",
    "FLAGS.add_argument('--train_bn', action='store_true', default=False,\n",
    "                   help='train batch norm or not')\n",
    "# Other.\n",
    "FLAGS.add_argument('--cuda', action='store_true', default=True,\n",
    "                   help='use CUDA')\n",
    "FLAGS.add_argument('--init_dump', action='store_true', default=False,\n",
    "                   help='Initial model dump.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loader_caffe(path, batch_size, num_workers=4, pin_memory=False):\n",
    "    \"\"\"Legacy loader for caffe. Used with models loaded from caffe.\"\"\"\n",
    "    # Returns images in 256 x 256 to subtract given mean of same size.\n",
    "    return data.DataLoader(\n",
    "        datasets.ImageFolder(path,\n",
    "                             transforms.Compose([\n",
    "                                 Scale((256, 256)),\n",
    "                                 # transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                             ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "def train_loader_cropped(path, batch_size, num_workers=4, pin_memory=False):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return data.DataLoader(\n",
    "        datasets.ImageFolder(path,\n",
    "                             transforms.Compose([\n",
    "                                 Scale((224, 224)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize,\n",
    "                             ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "def test_loader_cropped(path, batch_size, num_workers=4, pin_memory=False):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    return data.DataLoader(\n",
    "        datasets.ImageFolder(path,\n",
    "                             transforms.Compose([\n",
    "                                 Scale((224, 224)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize,\n",
    "                             ])),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory)\n",
    "\n",
    "\n",
    "# Note: This might not be needed anymore given that this functionality exists in\n",
    "# the newer PyTorch versions.\n",
    "class Scale(object):\n",
    "    \"\"\"Rescale the input PIL.Image to the given size.\n",
    "    Args:\n",
    "        size (sequence or int): Desired output size. If size is a sequence like\n",
    "            (w, h), output size will be matched to this. If size is an int,\n",
    "            smaller edge of the image will be matched to this number.\n",
    "            i.e, if height > width, then image will be rescaled to\n",
    "            (size * height / width, size)\n",
    "        interpolation (int, optional): Desired interpolation. Default is\n",
    "            ``PIL.Image.BILINEAR``\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR):\n",
    "        assert isinstance(size, int) or (isinstance(\n",
    "            size, collections.Iterable) and len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL.Image): Image to be scaled.\n",
    "        Returns:\n",
    "            PIL.Image: Rescaled image.\n",
    "        \"\"\"\n",
    "        if isinstance(self.size, int):\n",
    "            w, h = img.size\n",
    "            if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                return img\n",
    "            if w < h:\n",
    "                ow = self.size\n",
    "                oh = int(self.size * h / w)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "            else:\n",
    "                oh = self.size\n",
    "                ow = int(self.size * w / h)\n",
    "                return img.resize((ow, oh), self.interpolation)\n",
    "        else:\n",
    "            return img.resize(self.size, self.interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager(object):\n",
    "    \"\"\"Handles training and pruning.\"\"\"\n",
    "\n",
    "    def __init__(self, args: Args, model, previous_masks, dataset2idx, dataset2biases):\n",
    "        self.args = args\n",
    "        self.cuda = args.cuda\n",
    "        self.model = model\n",
    "        self.dataset2idx = dataset2idx\n",
    "        self.dataset2biases = dataset2biases\n",
    "\n",
    "        if args.mode != 'check':\n",
    "            # Set up data loader, criterion, and pruner.\n",
    "            \n",
    "            train_loader = train_loader_cropped\n",
    "            test_loader = test_loader_cropped\n",
    "\n",
    "            self.train_data_loader = train_loader(\n",
    "                args.train_path, args.batch_size, pin_memory=args.cuda)\n",
    "            self.test_data_loader = test_loader(\n",
    "                args.test_path, args.batch_size, pin_memory=args.cuda)\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            self.pruner = SparsePruner(\n",
    "                self.model, self.args.prune_perc_per_layer, previous_masks,\n",
    "                self.args.train_biases, self.args.train_bn)\n",
    "\n",
    "\n",
    "    def eval(self, dataset_idx, biases=None):\n",
    "        \"\"\"Performs evaluation.\"\"\"\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.apply_mask(dataset_idx)\n",
    "        if biases is not None:\n",
    "            self.pruner.restore_biases(biases)\n",
    "\n",
    "        self.model.eval()\n",
    "        error_meter = None\n",
    "\n",
    "        print('Performing eval...')\n",
    "        for batch, label in tqdm(self.test_data_loader, desc='Eval'):\n",
    "            if self.cuda:\n",
    "                batch = batch.cuda()\n",
    "            batch = Variable(batch, volatile=True)\n",
    "\n",
    "            output = self.model(batch)\n",
    "\n",
    "            # Init error meter.\n",
    "            if error_meter is None:\n",
    "                topk = [1]\n",
    "                if output.size(1) > 5:\n",
    "                    topk.append(5)\n",
    "                error_meter = tnt.meter.ClassErrorMeter(topk=topk)\n",
    "            error_meter.add(output.data, label)\n",
    "\n",
    "        errors = error_meter.value()\n",
    "        print('Error: ' + ', '.join('@%s=%.2f' %\n",
    "                                    t for t in zip(topk, errors)))\n",
    "        if self.args.train_bn:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.train_nobn()\n",
    "        return errors\n",
    "\n",
    "    def do_batch(self, optimizer, batch, label):\n",
    "        \"\"\"Runs model for one batch.\"\"\"\n",
    "        if self.cuda:\n",
    "            batch = batch.cuda()\n",
    "            label = label.cuda()\n",
    "        batch = Variable(batch)\n",
    "        label = Variable(label)\n",
    "\n",
    "        # Set grads to 0.\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        # Do forward-backward.\n",
    "        output = self.model(batch)\n",
    "        self.criterion(output, label).backward()\n",
    "\n",
    "        # Set fixed param grads to 0.\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.make_grads_zero()\n",
    "\n",
    "        # Update params.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Set pruned weights to 0.\n",
    "        if not self.args.disable_pruning_mask:\n",
    "            self.pruner.make_pruned_zero()\n",
    "\n",
    "    def do_epoch(self, epoch_idx, optimizer):\n",
    "        \"\"\"Trains model for one epoch.\"\"\"\n",
    "        for batch, label in tqdm(self.train_data_loader, desc='Epoch: %d ' % (epoch_idx)):\n",
    "            self.do_batch(optimizer, batch, label)\n",
    "\n",
    "    def save_model(self, epoch, best_accuracy, errors, savename):\n",
    "        \"\"\"Saves model to file.\"\"\"\n",
    "        base_model = self.model\n",
    "\n",
    "        # Prepare the ckpt.\n",
    "        self.dataset2idx[self.args.dataset] = self.pruner.current_dataset_idx\n",
    "        self.dataset2biases[self.args.dataset] = self.pruner.get_biases()\n",
    "        ckpt = {\n",
    "            'args': self.args,\n",
    "            'epoch': epoch,\n",
    "            'accuracy': best_accuracy,\n",
    "            'errors': errors,\n",
    "            'dataset2idx': self.dataset2idx,\n",
    "            'previous_masks': self.pruner.current_masks,\n",
    "            'model': base_model,\n",
    "        }\n",
    "        if self.args.train_biases:\n",
    "            ckpt['dataset2biases'] = self.dataset2biases\n",
    "\n",
    "        # Save to file.\n",
    "        torch.save(ckpt, savename + '.pt')\n",
    "\n",
    "    def train(self, epochs, optimizer, save=True, savename='', best_accuracy=0):\n",
    "        \"\"\"Performs training.\"\"\"\n",
    "        best_accuracy = best_accuracy\n",
    "        error_history = []\n",
    "\n",
    "        if self.args.cuda:\n",
    "            self.model = self.model.cuda()\n",
    "\n",
    "        for idx in range(epochs):\n",
    "            epoch_idx = idx + 1\n",
    "            print('Epoch: %d' % (epoch_idx))\n",
    "\n",
    "            optimizer = step_lr(epoch_idx, self.args.lr, self.args.lr_decay_every,\n",
    "                                      self.args.lr_decay_factor, optimizer)\n",
    "            if self.args.train_bn:\n",
    "                self.model.train()\n",
    "            else:\n",
    "                self.model.train_nobn()\n",
    "            self.do_epoch(epoch_idx, optimizer)\n",
    "            errors = self.eval(self.pruner.current_dataset_idx)\n",
    "            error_history.append(errors)\n",
    "            accuracy = 100 - errors[0]  # Top-1 accuracy.\n",
    "\n",
    "            # Save performance history and stats.\n",
    "            with open(savename + '.json', 'w') as fout:\n",
    "                json.dump({\n",
    "                    'error_history': error_history,\n",
    "                    'args': vars(self.args),\n",
    "                }, fout)\n",
    "\n",
    "            # Save best model, if required.\n",
    "            if save and accuracy > best_accuracy:\n",
    "                print('Best model so far, Accuracy: %0.2f%% -> %0.2f%%' %\n",
    "                      (best_accuracy, accuracy))\n",
    "                best_accuracy = accuracy\n",
    "                self.save_model(epoch_idx, best_accuracy, errors, savename)\n",
    "\n",
    "        print('Finished finetuning...')\n",
    "        print('Best error/accuracy: %0.2f%%, %0.2f%%' %\n",
    "              (100 - best_accuracy, best_accuracy))\n",
    "        print('-' * 16)\n",
    "\n",
    "    def prune(self):\n",
    "        \"\"\"Perform pruning.\"\"\"\n",
    "        print('Pre-prune eval:')\n",
    "        self.eval(self.pruner.current_dataset_idx)\n",
    "\n",
    "        self.pruner.prune()\n",
    "        self.check(True)\n",
    "\n",
    "        print('\\nPost-prune eval:')\n",
    "        errors = self.eval(self.pruner.current_dataset_idx)\n",
    "        accuracy = 100 - errors[0]  # Top-1 accuracy.\n",
    "        self.save_model(-1, accuracy, errors,\n",
    "                        self.args.save_prefix + '_postprune')\n",
    "\n",
    "        # Do final finetuning to improve results on pruned network.\n",
    "        if self.args.post_prune_epochs:\n",
    "            print('Doing some extra finetuning...')\n",
    "            optimizer = optim.SGD(self.model.parameters(),\n",
    "                                  lr=self.args.lr, momentum=0.9,\n",
    "                                  weight_decay=self.args.weight_decay)\n",
    "            self.train(self.args.post_prune_epochs, optimizer, save=True,\n",
    "                       savename=self.args.save_prefix + '_final', best_accuracy=accuracy)\n",
    "\n",
    "        print('-' * 16)\n",
    "        print('Pruning summary:')\n",
    "        self.check(True)\n",
    "        print('-' * 16)\n",
    "\n",
    "    def check(self, verbose=False):\n",
    "        \"\"\"Makes sure that the layers are pruned.\"\"\"\n",
    "        print('Checking...')\n",
    "        for layer_idx, module in enumerate(self.model.shared.modules()):\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                weight = module.weight.data\n",
    "                num_params = weight.numel()\n",
    "                num_zero = weight.view(-1).eq(0).sum()\n",
    "                if verbose:\n",
    "                    print('Layer #%d: Pruned %d/%d (%.2f%%)' %\n",
    "                          (layer_idx, num_zero, num_params, 100 * num_zero / num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
