{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, model_type=\"resnet18\", num_classes=10):\n",
    "        super(Expert, self).__init__()\n",
    "        if model_type == \"resnet18\":\n",
    "            self.model = models.resnet18(pretrained=True)\n",
    "            self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        elif model_type == \"vgg16\":\n",
    "            self.model = models.vgg16(pretrained=True)\n",
    "            self.model.classifier[-1] = nn.Linear(self.model.classifier[-1].in_features, num_classes)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Choose 'resnet18' or 'vgg16'\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.model(x), dim=-1)\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, n_experts, input_dim=512):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, n_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "class MOEModel(nn.Module):\n",
    "    def __init__(self, n_experts=3, model_type=\"resnet18\", num_classes=10, lr=0.001):\n",
    "        super(MOEModel, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(model_type, num_classes) for _ in range(n_experts)])\n",
    "        self.gate = Gate(n_experts, 512 if model_type == \"resnet18\" else 4096)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gate_outputs = self.gate(x).unsqueeze(-1)  # (batch_size, n_experts, 1)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)  # (batch_size, num_classes, n_experts)\n",
    "        return torch.matmul(expert_outputs, gate_outputs).squeeze(-1)  # (batch_size, num_classes)\n",
    "    \n",
    "    def probabilities(self, x, y):\n",
    "        expert_outputs = self.forward(x)\n",
    "        return torch.sum(y * expert_outputs, dim=1)\n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        probs = self.probabilities(x, y)\n",
    "        return -torch.log(probs + 1e-9).mean()\n",
    "    \n",
    "    def grad(self, x, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.calculate_loss(x, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def step(self, x, y):\n",
    "        loss = self.grad(x, y)\n",
    "        self.optimizer.step()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SparsePruner:\n",
    "    \"\"\"Performs pruning on the experts in the MOE model.\"\"\"\n",
    "\n",
    "    def __init__(self, model, prune_perc, previous_masks, train_bias, train_bn):\n",
    "        self.model = model\n",
    "        self.prune_perc = prune_perc\n",
    "        self.train_bias = train_bias\n",
    "        self.train_bn = train_bn\n",
    "        self.current_masks = None\n",
    "        self.previous_masks = previous_masks\n",
    "        \n",
    "        # Identify dataset index from the previous masks\n",
    "        valid_key = list(previous_masks.keys())[0]\n",
    "        self.current_dataset_idx = previous_masks[valid_key].max()\n",
    "    \n",
    "    def pruning_mask(self, weights, previous_mask, layer_idx):\n",
    "        \"\"\"Computes a pruning mask based on weight magnitudes.\"\"\"\n",
    "        previous_mask = previous_mask.cuda()\n",
    "        tensor = weights[previous_mask.eq(self.current_dataset_idx)]\n",
    "        abs_tensor = tensor.abs()\n",
    "        cutoff_rank = round(self.prune_perc * tensor.numel())\n",
    "        cutoff_value = abs_tensor.view(-1).cpu().kthvalue(cutoff_rank)[0].item()\n",
    "\n",
    "        remove_mask = weights.abs().le(cutoff_value) * previous_mask.eq(self.current_dataset_idx)\n",
    "        previous_mask[remove_mask.eq(1)] = 0\n",
    "        mask = previous_mask\n",
    "        \n",
    "        print(f'Layer #{layer_idx}, pruned {mask.eq(0).sum()}/{tensor.numel()} ({100 * mask.eq(0).sum() / tensor.numel():.2f}%)')\n",
    "        return mask\n",
    "    \n",
    "    def prune(self):\n",
    "        \"\"\"Prunes only the expert networks while keeping the gating network untouched.\"\"\"\n",
    "        print(f'Pruning experts for dataset idx: {self.current_dataset_idx}')\n",
    "        assert self.current_masks is None, 'Pruning twice?'\n",
    "        self.current_masks = {}\n",
    "        \n",
    "        print(f'Pruning each expert layer by removing {100 * self.prune_perc:.2f}% of values')\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    mask = self.pruning_mask(module.weight.data, self.previous_masks[(expert_idx, layer_idx)], layer_idx)\n",
    "                    self.current_masks[(expert_idx, layer_idx)] = mask.cuda()\n",
    "                    module.weight.data[self.current_masks[(expert_idx, layer_idx)].eq(0)] = 0.0\n",
    "    \n",
    "    def make_grads_zero(self):\n",
    "        \"\"\"Sets gradients of pruned weights to zero for the expert networks.\"\"\"\n",
    "        assert self.current_masks is not None\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    layer_mask = self.current_masks[(expert_idx, layer_idx)]\n",
    "                    \n",
    "                    if module.weight.grad is not None:\n",
    "                        module.weight.grad.data[layer_mask.ne(self.current_dataset_idx)] = 0\n",
    "                        if not self.train_bias and module.bias is not None:\n",
    "                            module.bias.grad.data.fill_(0)\n",
    "\n",
    "    def make_pruned_zero(self):\n",
    "        \"\"\"Forces pruned weights to remain zero in the expert networks.\"\"\"\n",
    "        assert self.current_masks is not None\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    module.weight.data[self.current_masks[(expert_idx, layer_idx)].eq(0)] = 0.0\n",
    "\n",
    "    def apply_mask(self, dataset_idx):\n",
    "        \"\"\"Applies the pruning mask for a specific dataset.\"\"\"\n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    weight = module.weight.data\n",
    "                    mask = self.previous_masks[(expert_idx, layer_idx)].cuda()\n",
    "                    weight[mask.eq(0)] = 0.0\n",
    "                    weight[mask.gt(dataset_idx)] = 0.0\n",
    "\n",
    "    def make_finetuning_mask(self):\n",
    "        \"\"\"Allows previously pruned weights to be trainable for the new dataset.\"\"\"\n",
    "        assert self.previous_masks is not None\n",
    "        self.current_dataset_idx += 1\n",
    "        \n",
    "        for expert_idx, expert in enumerate(self.model.experts):\n",
    "            for layer_idx, module in enumerate(expert.modules()):\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    mask = self.previous_masks[(expert_idx, layer_idx)]\n",
    "                    mask[mask.eq(0)] = self.current_dataset_idx\n",
    "        \n",
    "        self.current_masks = self.previous_masks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
