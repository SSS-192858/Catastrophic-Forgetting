{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Feature Extractor (ResNet-18)\n",
    "# ----------------------------\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Gating Network (Expert Selection)\n",
    "# ----------------------------\n",
    "class GatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        # Gumbel-Softmax for hard expert selection\n",
    "        gumbel_noise = -torch.log(-torch.log(torch.rand_like(logits) + 1e-10) + 1e-10)\n",
    "        expert_scores = F.softmax((logits + gumbel_noise) / 0.5, dim=1)\n",
    "\n",
    "        # Select the highest scoring expert\n",
    "        chosen_expert = torch.argmax(expert_scores, dim=1)\n",
    "\n",
    "        # One-hot encoding of the chosen expert\n",
    "        hard_expert = F.one_hot(chosen_expert, num_classes=expert_scores.shape[1]).float()\n",
    "\n",
    "        return hard_expert, expert_scores\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Masked Classifier (Neuron Masking)\n",
    "# ----------------------------\n",
    "class MaskedFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MaskedFeatureExtractor, self).__init__()\n",
    "        \n",
    "        # Feature extractor layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Store mask per task\n",
    "        self.masks = []\n",
    "\n",
    "    def apply_mask(self, x, task_id):\n",
    "        \"\"\"\n",
    "        Apply the mask corresponding to the current task.\n",
    "        \"\"\"\n",
    "        mask = self.masks[task_id] if task_id < len(self.masks) else torch.ones_like(x)\n",
    "        return x * mask\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.apply_mask(x, task_id)  # Apply task-specific mask\n",
    "        return x\n",
    "\n",
    "class TaskSpecificClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super(TaskSpecificClassifier, self).__init__()\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.fc2(x), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_neurons(model, prune_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Prune neurons based on magnitude and create a mask for the task.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The classifier being pruned\n",
    "    - prune_percentage: Percentage of neurons to prune\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            # if 'fc1.weight' in name:\n",
    "                # Get the absolute values of the weights\n",
    "                weights = torch.abs(param.data)\n",
    "\n",
    "                # Calculate threshold\n",
    "                threshold = torch.quantile(weights, prune_percentage)\n",
    "\n",
    "                # Create mask\n",
    "                mask = (weights > threshold).float()\n",
    "\n",
    "                # Store the mask\n",
    "                model.masks.append(mask)\n",
    "                \n",
    "                # Zero out pruned connections\n",
    "                param.data *= mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim, num_experts=3):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.gating_network = GatingNetwork(feature_dim, num_experts)\n",
    "        self.masked_feature_extractor = MaskedFeatureExtractor(feature_dim, hidden_dim)\n",
    "        \n",
    "        self.num_experts = num_experts\n",
    "        self.classifiers = nn.ModuleDict()  # Store different classifiers per task\n",
    "\n",
    "    def add_task(self, task_id, output_dim):\n",
    "        \"\"\"\n",
    "        Add a new task with a task-specific classifier.\n",
    "        \"\"\"\n",
    "        self.classifiers[str(task_id)] = TaskSpecificClassifier(hidden_dim=self.masked_feature_extractor.fc1.out_features, output_dim=output_dim)\n",
    "        self.classifiers[str(task_id)] = self.classifiers[str(task_id)].to(device)\n",
    "\n",
    "    def forward(self, x, task_id):\n",
    "        # 1. Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "\n",
    "        # 2. Select expert via gating network\n",
    "        hard_expert, expert_scores = self.gating_network(features)\n",
    "\n",
    "        # 3. Apply mask to the extracted features\n",
    "        masked_features = self.masked_feature_extractor(features, task_id)\n",
    "\n",
    "        # 4. Classify using task-specific classifier\n",
    "        if str(task_id) in self.classifiers:\n",
    "            outputs = self.classifiers[str(task_id)](masked_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Classifier for task {task_id} not found!\")\n",
    "\n",
    "        return outputs, expert_scores, hard_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_training(model, dataloader, criterion, optimizer, device, num_epochs=10, task_id=0, log_interval=100):\n",
    "    \"\"\"\n",
    "    Train the classifier fully on the current task before pruning.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0  # For tracking accuracy\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, _, _ = model(images, task_id)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Track accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Log periodically\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f\"Batch [{batch_idx}/{len(dataloader)}] (Task {task_id + 1}) - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch-level logging\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] (Task {task_id + 1}) - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_and_freeze(model, prune_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Prune the model and freeze the mask for Task 1.\n",
    "    \"\"\"\n",
    "    print(\"Pruning and freezing neurons for Task 1...\")\n",
    "    prune_neurons(model.classifier, prune_percentage)\n",
    "\n",
    "def retrain_with_mask(model, dataloader, criterion, optimizer, device, task_id=0, num_epochs=5, log_interval=10):\n",
    "    \"\"\"\n",
    "    Retrain the model on the given task using the frozen mask.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0  # For accuracy tracking\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with mask applied\n",
    "            outputs, _, _ = model(images, task_id)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Track accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # Log periodically\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(f\"Batch [{batch_idx}/{len(dataloader)}] (Task {task_id + 1}) - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch-level logging\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] (Task {task_id + 1}) - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/siddharth/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Device\n",
    "\n",
    "# Initialize model\n",
    "model = MixtureOfExperts(feature_dim=512, hidden_dim=256, num_experts=3).to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.Black_footed_Albatross', '002.Laysan_Albatross', '003.Sooty_Albatross', '004.Groove_billed_Ani', '005.Crested_Auklet', '006.Least_Auklet', '007.Parakeet_Auklet', '008.Rhinoceros_Auklet', '009.Brewer_Blackbird', '010.Red_winged_Blackbird', '011.Rusty_Blackbird', '012.Yellow_headed_Blackbird', '013.Bobolink', '014.Indigo_Bunting', '015.Lazuli_Bunting', '016.Painted_Bunting', '017.Cardinal', '018.Spotted_Catbird', '019.Gray_Catbird', '020.Yellow_breasted_Chat', '021.Eastern_Towhee', '022.Chuck_will_Widow', '023.Brandt_Cormorant', '024.Red_faced_Cormorant', '025.Pelagic_Cormorant', '026.Bronzed_Cowbird', '027.Shiny_Cowbird', '028.Brown_Creeper', '029.American_Crow', '030.Fish_Crow', '031.Black_billed_Cuckoo', '032.Mangrove_Cuckoo', '033.Yellow_billed_Cuckoo', '034.Gray_crowned_Rosy_Finch', '035.Purple_Finch', '036.Northern_Flicker', '037.Acadian_Flycatcher', '038.Great_Crested_Flycatcher', '039.Least_Flycatcher', '040.Olive_sided_Flycatcher', '041.Scissor_tailed_Flycatcher', '042.Vermilion_Flycatcher', '043.Yellow_bellied_Flycatcher', '044.Frigatebird', '045.Northern_Fulmar', '046.Gadwall', '047.American_Goldfinch', '048.European_Goldfinch', '049.Boat_tailed_Grackle', '050.Eared_Grebe', '051.Horned_Grebe', '052.Pied_billed_Grebe', '053.Western_Grebe', '054.Blue_Grosbeak', '055.Evening_Grosbeak', '056.Pine_Grosbeak', '057.Rose_breasted_Grosbeak', '058.Pigeon_Guillemot', '059.California_Gull', '060.Glaucous_winged_Gull', '061.Heermann_Gull', '062.Herring_Gull', '063.Ivory_Gull', '064.Ring_billed_Gull', '065.Slaty_backed_Gull', '066.Western_Gull', '067.Anna_Hummingbird', '068.Ruby_throated_Hummingbird', '069.Rufous_Hummingbird', '070.Green_Violetear', '071.Long_tailed_Jaeger', '072.Pomarine_Jaeger', '073.Blue_Jay', '074.Florida_Jay', '075.Green_Jay', '076.Dark_eyed_Junco', '077.Tropical_Kingbird', '078.Gray_Kingbird', '079.Belted_Kingfisher', '080.Green_Kingfisher', '081.Pied_Kingfisher', '082.Ringed_Kingfisher', '083.White_breasted_Kingfisher', '084.Red_legged_Kittiwake', '085.Horned_Lark', '086.Pacific_Loon', '087.Mallard', '088.Western_Meadowlark', '089.Hooded_Merganser', '090.Red_breasted_Merganser', '091.Mockingbird', '092.Nighthawk', '093.Clark_Nutcracker', '094.White_breasted_Nuthatch', '095.Baltimore_Oriole', '096.Hooded_Oriole', '097.Orchard_Oriole', '098.Scott_Oriole', '099.Ovenbird', '100.Brown_Pelican', '101.White_Pelican', '102.Western_Wood_Pewee', '103.Sayornis', '104.American_Pipit', '105.Whip_poor_Will', '106.Horned_Puffin', '107.Common_Raven', '108.White_necked_Raven', '109.American_Redstart', '110.Geococcyx', '111.Loggerhead_Shrike', '112.Great_Grey_Shrike', '113.Baird_Sparrow', '114.Black_throated_Sparrow', '115.Brewer_Sparrow', '116.Chipping_Sparrow', '117.Clay_colored_Sparrow', '118.House_Sparrow', '119.Field_Sparrow', '120.Fox_Sparrow', '121.Grasshopper_Sparrow', '122.Harris_Sparrow', '123.Henslow_Sparrow', '124.Le_Conte_Sparrow', '125.Lincoln_Sparrow', '126.Nelson_Sharp_tailed_Sparrow', '127.Savannah_Sparrow', '128.Seaside_Sparrow', '129.Song_Sparrow', '130.Tree_Sparrow', '131.Vesper_Sparrow', '132.White_crowned_Sparrow', '133.White_throated_Sparrow', '134.Cape_Glossy_Starling', '135.Bank_Swallow', '136.Barn_Swallow', '137.Cliff_Swallow', '138.Tree_Swallow', '139.Scarlet_Tanager', '140.Summer_Tanager', '141.Artic_Tern', '142.Black_Tern', '143.Caspian_Tern', '144.Common_Tern', '145.Elegant_Tern', '146.Forsters_Tern', '147.Least_Tern', '148.Green_tailed_Towhee', '149.Brown_Thrasher', '150.Sage_Thrasher', '151.Black_capped_Vireo', '152.Blue_headed_Vireo', '153.Philadelphia_Vireo', '154.Red_eyed_Vireo', '155.Warbling_Vireo', '156.White_eyed_Vireo', '157.Yellow_throated_Vireo', '158.Bay_breasted_Warbler', '159.Black_and_white_Warbler', '160.Black_throated_Blue_Warbler', '161.Blue_winged_Warbler', '162.Canada_Warbler', '163.Cape_May_Warbler', '164.Cerulean_Warbler', '165.Chestnut_sided_Warbler', '166.Golden_winged_Warbler', '167.Hooded_Warbler', '168.Kentucky_Warbler', '169.Magnolia_Warbler', '170.Mourning_Warbler', '171.Myrtle_Warbler', '172.Nashville_Warbler', '173.Orange_crowned_Warbler', '174.Palm_Warbler', '175.Pine_Warbler', '176.Prairie_Warbler', '177.Prothonotary_Warbler', '178.Swainson_Warbler', '179.Tennessee_Warbler', '180.Wilson_Warbler', '181.Worm_eating_Warbler', '182.Yellow_Warbler', '183.Northern_Waterthrush', '184.Louisiana_Waterthrush', '185.Bohemian_Waxwing', '186.Cedar_Waxwing', '187.American_Three_toed_Woodpecker', '188.Pileated_Woodpecker', '189.Red_bellied_Woodpecker', '190.Red_cockaded_Woodpecker', '191.Red_headed_Woodpecker', '192.Downy_Woodpecker', '193.Bewick_Wren', '194.Cactus_Wren', '195.Carolina_Wren', '196.House_Wren', '197.Marsh_Wren', '198.Rock_Wren', '199.Winter_Wren', '200.Common_Yellowthroat']\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "train_dataset = datasets.ImageFolder(root='cubs_cropped/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.ImageFolder(root='cubs_cropped/test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0/188] (Task 1) - Loss: 5.2985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m task_1_output_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_names)  \u001b[38;5;66;03m# Number of classes for task 1\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd_task(task_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mtask_1_output_dim)  \u001b[38;5;66;03m# Add task-specific classifier\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mfull_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Task 1: Pruning and freezing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m prune_and_freeze(model, prune_percentage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, task_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mfull_training\u001b[0;34m(model, dataloader, criterion, optimizer, device, num_epochs, task_id, log_interval)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Track accuracy\u001b[39;00m\n\u001b[1;32m     27\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Task 1: Full training\n",
    "task_1_output_dim = len(class_names)  # Number of classes for task 1\n",
    "\n",
    "model.add_task(task_id=0, output_dim=task_1_output_dim)  # Add task-specific classifier\n",
    "full_training(model, train_loader, criterion, optimizer, device, num_epochs=10, task_id=0)\n",
    "\n",
    "# Task 1: Pruning and freezing\n",
    "prune_and_freeze(model, prune_percentage=0.2, task_id=0)\n",
    "\n",
    "# Task 1: Retraining with frozen mask\n",
    "retrain_with_mask(model, train_loader, criterion, optimizer, device, task_id=0, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Task 1: Full training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfull_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Task 1: Pruning and freezing\u001b[39;00m\n\u001b[1;32m      4\u001b[0m prune_and_freeze(model, prune_percentage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mfull_training\u001b[0;34m(model, dataloader, criterion, optimizer, device, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "train_dataset = datasets.ImageFolder(root='flowers/train', transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = datasets.ImageFolder(root='flowers/test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)\n",
    "\n",
    "# Task 2: Repeat the same pipeline (full training, pruning, and retraining)\n",
    "task_2_output_dim = len(class_names)  # Number of classes for task 2 (change if needed)\n",
    "model.add_task(task_id=1, output_dim=task_2_output_dim)  # Add classifier for Task 2\n",
    "full_training(model, train_loader, criterion, optimizer, device, num_epochs=10, task_id=1)\n",
    "prune_and_freeze(model, prune_percentage=0.2, task_id=1)\n",
    "retrain_with_mask(model, train_loader, criterion, optimizer, device, task_id=1, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Task 2: Repeat the same pipeline (full training, pruning, and retraining)\n",
    "# full_training(model, train_loader, criterion, optimizer, device, num_epochs=10)\n",
    "# prune_and_freeze(model, prune_percentage=0.75)\n",
    "# retrain_with_mask(model, train_loader, criterion, optimizer, device, task_id=1, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
